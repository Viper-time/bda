{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93495c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2) Given two RDD AND perform all joint operation \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e37e0aa-0822-42d1-b9ae-e70e1eb6c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out path where pyspark installed\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de3afa6-e29e-4e04-ba73-a8f42c23b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession and sparkcontext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local\")\\\n",
    "                    .appName('Firstprogram')\\\n",
    "                    .getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800149c2-bae8-44a2-bc4a-831f13a84ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hadoop', (4, 5)), ('spark', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "# Create Context and Session for Spark from pyspark.context import SparkContext from pyspark.sql.session import SparkSession sc=SparkContext.getOrCreate() spark=SparkSession(sc)\n",
    "#Using parallelize method, create RDD\n",
    "rdd1=sc.parallelize([(\"spark\", 1),(\"hadoop\", 4)])\n",
    "rdd2=sc.parallelize([(\"spark\", 2),(\"hadoop\", 5)])\n",
    "\n",
    "#Perforn Join Operation on the created RDDs\n",
    "rdd=sorted(rdd1.join(rdd2).collect())\n",
    "#Print the Result\n",
    "print(rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc98a4c-e90f-4bd1-9e6d-5932539cc812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hadoop', (4, 5)), ('spark', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "rdd3=sorted(rdd1.fullOuterJoin(rdd2).collect())\n",
    "print(rdd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ebe357-f74a-4cad-9434-9d1d790f3149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hadoop', (4, 5)), ('spark', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(rdd1.leftOuterJoin(rdd2).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "662fae51-4913-49d8-95ef-d92302cc1f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "rdd2 = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(rdd1.join(rdd2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af256aea-fb3b-412d-9f25-32186d4f6c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('b', (4, None))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "rdd2 = sc.parallelize([(\"a\", 2)])\n",
    "sorted(rdd1.leftOuterJoin(rdd2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90a0ed7-a3da-4f31-9dc9-763c895ebf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (2, 1)), ('b', (None, 4))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "rdd2 = sc.parallelize([(\"a\", 2)])\n",
    "sorted(rdd2.rightOuterJoin(rdd1).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2d93f32-b7b2-4c9b-9cf7-90b8da8b8d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "rdd2 = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
    "sorted(rdd1.fullOuterJoin(rdd2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523f51cf-9342-418a-8a03-4f5dc3e78494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import desc\n",
    "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
    "df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
    "df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
    "df4 = spark.createDataFrame([\n",
    "    Row(age=10, height=80, name=\"Alice\"),\n",
    "    Row(age=5, height=None, name=\"Bob\"),\n",
    "    Row(age=None, height=None, name=\"Tom\"),\n",
    "    Row(age=None, height=None, name=None),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c05cb893-0556-4ced-b3c8-6e5fa756dfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name|height|\n",
      "+----+------+\n",
      "| Bob|    85|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df2, 'name').select(df.name, df2.height).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8275f35e-6db1-43e1-8dc9-392c5c83aa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|height|\n",
      "+-----+------+\n",
      "|  Bob|    85|\n",
      "|Alice|  NULL|\n",
      "| NULL|    80|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df2, df.name == df2.name, 'outer').select(\n",
    "    df.name, df2.height).sort(desc(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f535e-b35d-4e6b-a4b9-be0281afd959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
